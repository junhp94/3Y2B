import torchaudio
# import soundfile as sf
import glob, os, time
from speechbrain.inference.speaker import SpeakerRecognition
verification = SpeakerRecognition.from_hparams(source="pretrained_models/")

# experimentation - instead of loading the audio files
# (actual files stored at file path), can load
# waveforms generated by torchaudio.load(bytes) as well.
# audio data is in tensor form and can be serialized / saved
os.chdir("./")
fnList = []
t1 = time.perf_counter()
for file in glob.glob("*.wav"):
    data, sample_rate = torchaudio.load(file)
    fnList.append((file, data))

for idx, item in enumerate(fnList):
    for f2 in fnList[:idx] + fnList[idx + 1:]:
        score, prediction = verification.verify_batch(fnList[idx][1], f2[1]) # Different Speakers
        print('{} - {}: {}, {}'.format(fnList[idx][0], f2[0], score, prediction))

t2 = time.perf_counter()
print(f"Time taken -  {t2 - t1:0.4f} seconds")

#score, prediction = verification.verify_files("test1.wav", "test2.wav") # Different Speakers
#print('{}, {}'.format(score, prediction))
#score, prediction = verification.verify_files("test1.wav", "test3.wav") # Same Speaker
#print('{}, {}'.format(score, prediction))

#signal, fs = torchaudio.load('test1.wav')
#embeddings = classifier.encode_batch(signal)
#print(embeddings)